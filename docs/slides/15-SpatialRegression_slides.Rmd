---
title: "Пространственная и географически взвешенная регрессия"
subtitle: "Пространственная статистика"
author: "Тимофей Самсонов"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default, "style.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(raster)
library(gstat)
library(stars)
library(tmap)
library(sf)
library(sp)
```

## Особенности случайных процессов в пространстве

- __Пространственная зависимость__ _(spatial dependence)_ — наличие автокорреляции наблюдений. Выражается в невыполнении условия независимости остатков линейной регрессии. Устраняется посредством _пространственной регрессии (spatial regression)_.

- __Пространственная гетерогенность__ _(spatial heterogeneity)_ — нестационарность процессов, порождающих наблюдаемую переменную. Выражается в неэффективности постоянных коэффициентов линейной регрессии. Устраняется постредством _географически взвешенной регрессии (geographically weighted regression)_.

---

## Линейная регрессия

Пусть дан вектор $\mathbf{y} = \{y_1, y_2, ... y_n\}$ измерений зависимой переменной, а также матрица $\mathbf{X} = \{x_{ij}\}$ размером $n \times m$, состоящая из значений $m$ независимых переменных для $n$ измерений. В этом случае модель линейной регрессии может быть записана как

$$\mathbf{y} = \mathbf{X} \boldsymbol\beta + \boldsymbol\epsilon,$$

где:

- $\boldsymbol\beta$ — вектор коэффициентов регрессии;

- $\boldsymbol\epsilon$ — вектор случайных ошибок, независимо распределенных относительно среднего значения в нуле.

---

## Многомерное нормальное распределение

Многомерное нормальное распределение (МНР) $k$-мерного случайного вектора $\mathbf{X} = (X_1, ..., X_k)^T$ обозначается как:

$$\mathbf{X}\ \sim \mathcal{N}_k(\boldsymbol\mu,\, \boldsymbol\Sigma)$$

МНР определяется двумя параметрами:

- __математическое ожидание__ ( $k$-мерный вектор):

$$\boldsymbol\mu = \operatorname{E}[\mathbf{X}] = [ \operatorname{E}[X_1], \operatorname{E}[X_2], \ldots, \operatorname{E}[X_k]]^{\rm T}$$

- __ковариационная матрица__ (размером $k \times k$):

$$\boldsymbol\Sigma = \operatorname{E} [(\mathbf{X} - \boldsymbol\mu)( \mathbf{X} - \boldsymbol \mu)^{\rm T}] =  [ \operatorname{Cov}[X_i, X_j]; 1 \le i,j \le k ]$$

---

### Стандартный нормальный случайный вектор

Вещественнозначный случайный вектор $\mathbf{X} = (X_1, ..., X_k)^T$ называется __стандартным нормальным случайным вектором__, если все его компоненты $X_n$ независимы друг от друга и подчиняются стандартному случаю нормального закона распределения с нулевым математическим ожиданием и единичной дисперсией для всех $n$:

$$X_n \sim \mathcal{N}(0, 1)$$ 

В модели линейной регрессии:

$$\boldsymbol\epsilon \sim \mathcal{N}_k(0, \sigma^2 \mathbf{I}),$$

где $I$ — единичная матрица размером $k \times k$.

---

### Расширение класса регрессионных моделей

$$\boldsymbol\epsilon \sim \mathcal{N}_k(0, \sigma^2 \mathbf{I})$$

Однако если данные получены измерениями по пространству, остатки регрессии могут демонстрировать пространственную ассоциацию (зависимость), как правило свидетельствующую о наличии дополнительных неучтённых факторов. Это означает, что обычная модель регрессии недостаточно хорошо объясняет зависимость.

Чтобы моделировать зависимость остатков, необходим более широкий класс моделей:

$$\boldsymbol\epsilon \sim \mathcal{N}_k(0, \mathbf{C}),$$

где $\mathbf{C}$ — любая допустимая ковариационная матрица.

---

## Пример

Процент домохозяйств, находящихся во владении

![:scale 65%](images/tyne_ownerocc.png)

---

## Пример

Уровень безработицы

![:scale 65%](images/tyne_unempl.png)

---

## Пример

Обычная линейная регрессия

![:scale 75%](images/tyne_regr.png)

---

## Пример

Остатки регрессии

![:scale 65%](images/tyne_resid.png)


---

### Расширение класса регрессионных моделей

$$\boldsymbol\epsilon \sim \mathcal{N}_k(0, \mathbf{C})$$

Данная модель решает проблему независимости остатков, однако порождает две других проблемы:

- Если зависимость остатков имеет пространственный характер (ассоциированы остатки в территориально близких локациях), то матрица $\mathbf{C}$ характер этой зависимости не отражает в явном виде.

- Вектор коэффициентов регрессии $\boldsymbol\beta$ может быть получен путем минимизации $\mathbf{y} - \mathbf{X}\boldsymbol\beta$ путем решения $\beta = \big(\mathbf{X}^T \mathbf{CX} \big)^{-1} \mathbf{X}^T \mathbf{X y}$. Однако это требует знания ковариационной матрицы, которая обычно неизвестна. Поэтому как $\mathbf{C}$, так и $\boldsymbol\beta$ калибруются по выборке.

---

## Пространственная регрессия

Для того чтобы учесть пространственную автокорреляцию остатков, в модель линейной регрессии добавляется компонента __пространственной авторегрессии__ _(spatial autoregression)_, которая моделирует _пространстенный лаг_:

$$\mathbf{y} = \mathbf{X} \mathbf{\beta} + \color{red}{\rho\mathbf{Wy}} +  \mathbf{\epsilon},$$

- $\rho$ — коэффициент регрессии, отражающий степень пространственной автокорреляции

- $\mathbf{W}$ — матрица пространственных весов

> Полученная модель называется моделью __пространственной регрессии__ (_spatial regression_).

---

## Пространственная регрессия

Для получения коэффициентов $\boldsymbol\beta$ и $\rho$ выполним ряд преобразований:

$$\mathbf{y} = \mathbf{X} \mathbf{\beta} + \rho\mathbf{Wy} +  \mathbf{\epsilon}\\
\mathbf{y} - \rho\mathbf{Wy} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}\\
(\mathbf{I} - \rho\mathbf{W})\mathbf{y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$$

Предполагая, что матрица $(\mathbf{I} - \rho\mathbf{W})$ инвертируема, получаем систему уравнений пространственной регрессии:

$$\color{red}{\boxed{\color{blue}{\mathbf{y} = (\mathbf{I} - \rho\mathbf{W})^{-1}\mathbf{X}\mathbf{\beta} + (\mathbf{I} - \rho\mathbf{W})^{-1}\mathbf{\epsilon}}}}$$

Данная модель идентична обычной регрессии $\mathbf{y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$, но в ней независимые переменные и ошибки линейно трансформированы умножением на $(\mathbf{I} - \rho\mathbf{W})^{-1}$.

---

## Пространственная регрессия

$$\mathbf{y} = (\mathbf{I} - \rho\mathbf{W})^{-1}\mathbf{X}\mathbf{\beta} + (\mathbf{I} - \rho\mathbf{W})^{-1}\mathbf{\epsilon}$$

Трансформированная ошибка модели будет иметь ковариационную матрицу

$$\mathbf{C} = \sigma^2 \Big[\big(\mathbf{I} - \rho \mathbf{W}\big)^{-1}\Big]^T (\mathbf{I} - \rho\mathbf{W})^{-1}$$

- Если ковариационная матрица функционально зависит от параметра $\rho$, то она отражает пространственную структуру автокорреляции ошибок.

- Ковариационная матрица должна быть положительно определенной. Для полученного выражения это будет выполняться в случае если $|\rho| \leq 1$ (Griffith, 1988).

---

## Пространственная регрессия

$$\mathbf{y} = \mathbf{X} \mathbf{\beta} + \rho\mathbf{Wy} +  \mathbf{\epsilon}$$

Для нахождения коэффициентов $\boldsymbol\beta$ и $\rho$ используется минимизация квадрата случайной компоненты, которую можно представить как $\mathbf{\epsilon} = \mathbf{y} - \mathbf{X} \mathbf{\beta} - \rho\mathbf{Wy}$:

$$\sum_i \Bigg(y_i - \sum_j \beta_j x_{ij} - \rho \sum_j w_{ij} y_j \Bigg)^2$$

Задача решается в 2 этапа:

- находится оптимальное значение $\rho$;
- находится оптимальное значение $\boldsymbol\beta$ путем подстановки в вышеуказанное выражение.

---

## Пространственная фильтрация

Модель пространственной регрессии может быть использована для осуществления __пространственной фильтрации__ — убирания автокорреляционной составляющей. Для этого необходимо авторегрессионную компоненту (пространственный лаг) перенести в левую часть уравнения:

$$\mathbf{y} = \mathbf{X} \mathbf{\beta} + \rho\mathbf{Wy} + \mathbf{\epsilon}\\
\mathbf{y}^* = \mathbf{y} - \rho\mathbf{Wy} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$$

Данная модель представляет собой стандартную (непространственную) регрессию для независимой переменной $\mathbf{y}^*$, в которой пространственная корреляция убрана (подвергнута фильтрации).

---

## Пространственная фильтрация

$$\mathbf{y}^* = \mathbf{y} - \rho\mathbf{Wy} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$$

- Пространственная фильтрация бывает полезна, когда наблюдается несоответствие масштаба наблюдений и масштаба процесса. 

- Например, статистика по показателю, контролируемому на региональном уровне, собирается по муниципалитетам. В этом случае фильтрация позволяет подобрать параметры $\mathbf{\beta}$, учитывающие наличие высокой пространственной автокорреляци.

---

## Географически взвешенная регрессия

В стандартной модели линейной регрессии параметры модели предполагаются постоянными:

$$\mathbf{y} = \mathbf{X} \boldsymbol\beta + \boldsymbol\epsilon,$$

Для $i$-й локации решению выглядит следующим образом:

$$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_m x_{mi} + \epsilon_i$$

Коэффициенты находятся методом наименьших квадратов:

$$\mathbf{\beta}' = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}$$

Такой подход, однако не учитывает того, что характер зависимости между переменными может меняться по пространству.

---

## Географически взвешенная регрессия

В географически взвешенной регрессионной модели веса определяются для каждой локации:

$$y_i = \beta_{0i} + \beta_{1i} x_{1i} + \beta_{2i} x_{2i} + ... + \beta_{mi} x_{mi} + \epsilon_i$$

В этом случае область оценки параметров $\mathbf{\beta}$ ограничивается некой окрестностью точки $i$. Математически это достигается применением весовых коэффициентов для данных независимых переменных:

$$\mathbf{\beta}'(i) = (\mathbf{X}^T \color{blue}{\mathbf{W}(i)}\mathbf{X})^{-1} \mathbf{X}^T \color{blue}{\mathbf{W}(i)} \mathbf{Y},$$

где $\mathbf{W}(i)$ есть матрица весов для точки $i$. Коэффициенты матрицы подбираются таким образом, что близкие локации получают более высокий вес.

---

## Географически взвешенная регрессия

Матрица $\mathbf{W}(i)$ имеет размер $n \times n$, где $n$ — число точек наблюдений:

$$\mathbf{W}(i) = \begin{bmatrix}
    w_{i1} & 0 & 0 & \dots  & 0 \\
    0 & w_{i2} & 0 & \dots  & 0 \\
    0 & 0 & w_{i3} & \dots  & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \dots  & w_{in}
\end{bmatrix},$$

где $w_{ik}$ есть вес, который имеет точка $k$ при локальной оценке параметров в точке $i$.

---

## Весовые функции

Весовая функция должна быть убывающей. Существует множество вариантов таких функций, но наиболее часто используются гауссоподобные варианты:

![:scale 65%](images/gwr_wlocal.png)

---

## Весовые функции

В случае фиксированной весовой функции окрестность всегда имеет фиксированный размер:

$$w_{ij} = \operatorname{exp}\{-\frac{1}{2} (d_{ij}/h)^2\},$$

где $d_{ij}$ есть расстояние, $h$ — полоса пропускания.

![:scale 75%](images/wfixed.png)

---

## Весовые функции

В случае адаптивной весовой функции окрестность ограничивается $N$ ближайшими точками. За пределами этой окрестности веса принимаются равными нулю:

![:scale 85%](images/wadaptive.png)

---

## Полоса пропускания

$$w_{ij} = \operatorname{exp}\{-\frac{1}{2} (d_{ij}/h)^2\},$$

__Полоса пропускания__ $h$ обладает следующими особенностями:

- малая полоса пропускания приводит к большой дисперсии локальных оценок;
- большая полоса пропускания приводит к смещенности оценки;
- при $h \rightarrow \infty$ локальная модель приближается к _глобальной регрессии_;
- при $h \rightarrow 0$ локальная модель «сворачивается» вокруг данных.
