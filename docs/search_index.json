[
["circular.html", "Глава 8 Статистика направлений и времени 8.1 Предварительные требования 8.2 Статистика направлений 8.3 Временные ряды 8.4 Контрольные вопросы и упражнения", " Глава 8 Статистика направлений и времени 8.1 Предварительные требования Для работы по теме текущей лекции вам понадобятся пакеты из tidyverse. Помимо этого, необходимы методы круговой статистики из пакетов circular и NPCirc, и методы из пакета pracma. Для работы с временными данными мы воспользуемся пакетом lubridate, который входит в tidyverse, но автоматически не подключается в сессию. Мы также воспользуемся пакетом gganimate, который позволяет анимировать графики, построенные с помощью ggplot: library(tidyverse) library(circular) library(readxl) library(NPCirc) library(pracma) library(lubridate) library(gganimate) 8.2 Статистика направлений 8.2.1 Теория 8.2.1.1 Распределение фон Мизеса В географии направления играют огромную роль. Ветер, морские течения, уличная сеть, перелеты птиц — все эти явления можно охарактеризовать их направленностью. Для того, чтобы эффективно анализировать такие данные, необходимо владеть специализированным математическим аппаратом. Обработкой данных о направлениях занимается особая область математической статистики — статистика направлений, или круговая (циркулярная) статистика (Mardia, Jupp, 2000; Pewsey et al., 2013). В круговой статистике каждое направление \\(\\theta \\in [0, 2\\pi)\\) представляется в виде вектора \\(x = (\\cos \\theta, sin \\theta)\\). Все операции производятся над подобными векторами и их координатами. Аналогом нормального распределения для круговой случайной величины является распределение фон Мизеса (von Mises, 1918), которое задается функцией плотности вероятности: \\[ f(θ)=\\frac{1}{2 \\pi I_0(\\kappa)} e^{\\kappa \\cos (\\theta - \\mu)}, \\] где \\(\\kappa \\geq 0\\) — параметр концентрации, \\(\\mu\\) — среднее значение (для \\(\\kappa &gt; 0\\)) и \\[ I_p(\\kappa) = \\frac{1}{2π} \\int_{0}^{2\\pi} \\cos (p \\theta) e^{\\kappa \\cos θ} d \\theta \\] есть модифицированная функция Бесселя первого рода и порядка \\(p\\). Из формул видно, что по своему эффекту параметр концентрации противоположен среднеквадратическому отклонению \\(\\sigma\\), которое является параметром нормального распределения. Чем больше значение \\(\\kappa\\), тем более сконцентрировано распределение относительно среднего значения — отсюда идет название этого параметра. Распределение фон Мизеса используется для построения ядра при аппроксимации плотности распределения направлений методом ядерной оценки (оценки по методу Парзена-Розенблатта). В метеорологии значения \\(\\cos \\theta\\) и \\(\\sin \\theta\\) определяют соотношение зональной и меридиональной составляющей скорости [ветра] (для получения самих составляющих их надо умножить на скорость ветра). 8.2.1.2 Вычисление статистических моментов Для вычисления статистических моментов круговой случайной величины требуется найти средний равнодействующий вектор первого порядка: \\[R = (C, S),\\] где \\[C = \\frac{1}{n} \\sum_{j=1}^{n} \\cos \\theta_j,\\\\ S = \\frac{1}{n} \\sum_{j=1}^{n} \\sin \\theta_j.\\] Данный вектор имеет направление \\(\\bar\\theta\\), которое является выборочным средним направлением исследуемой величины. Выборочная средняя равнодействующая длина \\(\\bar R = \\sqrt{C^2 + S^2}\\) принимает значения в диапазоне \\([0, 1]\\) и показывает меру концентрации направлений относительно \\(\\theta\\). \\(\\bar R = 1\\) означает, что все исходные направления совпадают, \\(\\bar R = 0\\) — что данные равномерно распределены по кругу, либо распределение имеет несколько мод, которые уравновешивают друг друга. Величина \\(\\bar R\\) дает важную информацию для предварительной диагностики картины направлений. Если значение \\(\\bar R\\) близко к единице, это означает, что распределение является унимодальным и в качестве основного направления можно принять значение \\(\\bar θ\\) (Mardia and Jupp 2000). Стандартное отклонение направлений \\(v\\) в радианах может быть найдено как \\(v=\\sqrt{-2 \\ln \\bar R}\\) . В ряде случаев противоположные направления считаются эквивалентными. Например, нельзя сказать, идет ли улица с юга на север или с севера на юг. Такие данные в теории круговой статистики называются аксиальными (Mardia, Jupp, 2000). Для аксиальных данных возможный диапазон значений лежит в интервале \\([0, \\pi)\\). Поскольку методы круговой статистики рассчитаны на круговое замыкание данных, стандартный подход к обработке аксиальных данных предполагает переход от направлений к их удвоенным значениям \\(\\theta&#39; = 2\\theta\\), обработку полученных значений стандартными методами и отображение полученных значение обратно на интервал \\([0, \\pi)\\). Для среднего, медианы и моды распределения это означает простое деление полученного значения пополам (Pewsey, Neuhäuser, and Ruxton 2013). 8.2.1.3 Определение модальных направлений Модальные направления могут быть определены как по гистограмме распределения, так и методом ядерной оценки. Основной вопрос поиска эффективного ядра заключается в параметризации функции \\(K\\). Для распределения фон Мизеса таким параметром является концентрация \\(\\kappa\\). Чем больше этот параметр, тем более локализованной будет оценка, тем сильнее будут проявляться в ней существующие моды распределения, но также будут и выделяться новые моды, которые на самом деле не значимы. Малые значения \\(\\kappa\\) приведут, наоборот, к «размыванию» плотности распределения в пределах полного круга. Как и в случае с количеством интервалов гистограммы, избыточно малые и большие значения κ нежелательны. В работе (Oliveira, Crujeiras, and Rod’riguez-Casal 2012) показано, что оптимальное значение \\(\\kappa\\) может быть подобрано также для оценки распределений, являющихся конечной суммой \\(M\\) распределений фон Мизеса, то есть, мультимодальных распределений, имеющих плотность : \\[g(\\theta)=\\sum_{i=1}^{M} \\alpha_i \\frac{\\exp\\lbrace{\\kappa_i \\cos(\\theta - \\mu_i)\\rbrace}}{2 \\pi I_0 (\\kappa_i)},\\] где \\(\\sum_{i=1}^{M} = 1\\). Поскольку в результате подбора определяется не только параметр концентрации, но и число компонент в сумме распределений (Oliveira, Crujeiras, and Rod’riguez-Casal 2014), его можно также использовать для определения количества искомых мод, если это необходимо. Когда подобрана функция ядра и ее параметры, оценка плотности распределения (вычисление функции \\(\\circ f _h (x)\\)) для круговых данных делается либо для исходных направлений \\(\\theta_j\\), либо с равным (достаточно малым) интервалом — например, через 1 градус (Pewsey, Neuhäuser, and Ruxton 2013). После того как произведена оценка, могут быть выбраны направления, в которых функция плотности распределения достигает локального максимума — первого и второго по величине. Эти направления и будут соответствовать первой и второй моде распределения направлений. 8.2.2 Практика В практической части данного раздела мы будем работать с массивом среднемесячных значений метеопараметров в пограничном слое атмосферы по полярным аэрологическим обсерваториям России. Массив данных ежемесячно обновляется на портале Аисори-М ВНИИГМИ-МЦД. В системе доступны данные по следующим обсерваториям: obs = readxl::read_excel(&#39;data/bound/scheme.xlsx&#39;, 2) Индекс Название Широта Долгота 20674 Остров Диксон 73.50 80.42 21824 Тикси 71.35 128.55 22113 Мурманск 68.59 33.07 22217 Кандалакша 67.09 32.21 22271 Шойна 67.53 44.09 23078 Норильск 69.20 88.18 23205 Нарьян-Мар 67.39 53.07 23330 Салехард 66.32 66.40 24125 Оленек 68.31 112.26 24266 Верхоянск 67.55 133.38 24343 Жиганск 66.46 123.21 89512 Новолазаревская -70.75 11.83 89592 Мирный -66.65 19.71 Для каждой обсерватории даны следующие параметры: Призначная часть/ метеоэлемент/число наблюдений Обозначение Число цифр Единицы измерения Константа отсутствия Индекс станции INDEX 5 - нет Год GGGG 5 - нет Месяц MM 3 - нет Срок HH 3 GMT нет Стандартное значение высоты Z 6 м нет Среднемесячные значения давления MP 6 10·гПа -9999 Среднеквадратические отклонения давления SP 6 10·гПа -9999 Число наблюдений для давления NP 3 - нет Среднемесячные значения температуры MT 6 10·°C -9999 Среднеквадратические отклонения температуры ST 6 10·°C -9999 Число наблюдений для температуры NT 3 - нет Среднемесячные значения дефицита точки росы MD 6 10·°C -9999 Среднеквадратические отклонения дефицита точки росы SD 6 10·°C -9999 Число наблюдений для дефицита точки росы ND 3 - нет Среднемесячные значения скалярной скорости ветра MS 6 10·м/с -9999 Среднеквадратические отклонения скалярной скорости ветра SS 6 10·м/с -9999 Число наблюдений для скалярной скорости ветра NS 3 - нет Среднемесячные значения зональной составляющей скорости ветра MU 6 10·м/с -9999 Среднеквадратические отклонения зональной составляющей скорости ветра SU 6 10·м/с -9999 Число наблюдений для зональной составляющей скорости ветра NU 3 - нет Среднемесячные значения меридиональной составляющей скорости ветра MV 6 10·м/с -9999 Среднеквадратические отклонения меридиональной составляющей скорости ветра SV 6 10·м/с -9999 Число наблюдений для меридиональной составляющей скорости ветра NV 3 - нет Загрузим данные по всем обсерваториям из текстовых файлов в папке bound: files = paste(&#39;data/bound&#39;, list.files(&#39;data/bound&#39;, &quot;*.txt&quot;), sep = &#39;/&#39;) (tab = lapply(files, function(X) { readr::read_table(X, col_names = params$Обозначение) }) %&gt;% bind_rows() %&gt;% left_join(obs, by = c(&#39;INDEX&#39; = &#39;Индекс&#39;))) # присоединим информацию о названиях станций ## # A tibble: 77,073 x 26 ## INDEX GGGG MM HH Z MP SP NP MT ST NT MD ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20674 2007 1 0 2000 7629 78 27 -187 35 27 53 ## 2 20674 2007 1 0 1900 7732 79 27 -182 36 27 52 ## 3 20674 2007 1 0 1800 7836 79 27 -178 36 27 51 ## 4 20674 2007 1 0 1700 7942 80 27 -173 36 27 49 ## 5 20674 2007 1 0 1600 8048 81 27 -168 36 27 48 ## 6 20674 2007 1 0 1500 8157 81 27 -164 38 27 47 ## 7 20674 2007 1 0 1400 8266 82 27 -160 39 27 45 ## 8 20674 2007 1 0 1300 8376 82 27 -156 39 27 43 ## 9 20674 2007 1 0 1200 8488 83 27 -152 40 27 40 ## 10 20674 2007 1 0 1100 8601 83 27 -148 41 27 37 ## # … with 77,063 more rows, and 14 more variables: SD &lt;dbl&gt;, ND &lt;dbl&gt;, ## # MS &lt;dbl&gt;, SS &lt;dbl&gt;, NS &lt;dbl&gt;, MU &lt;dbl&gt;, SU &lt;dbl&gt;, NU &lt;dbl&gt;, MV &lt;dbl&gt;, ## # SV &lt;dbl&gt;, NV &lt;dbl&gt;, Название &lt;chr&gt;, Широта &lt;dbl&gt;, Долгота &lt;dbl&gt; Создадим объект типа circular (из пакета circular) с направлениями ветра для анализа, и запишем его в новую переменую таблицы. Предварительно определим вспомогательную функцию, вычисляющую географический азимут на основе компонент скорости: geo_azimuth = function(dx, dy) { a = atan2(dx, dy) ifelse(a &lt;= pi/2, pi/2 - a, 5*pi/2 - a) } (winds = tab %&gt;% mutate(wind = circular(geo_azimuth(MV, MU), template = &#39;geographics&#39;)) %&gt;% select(INDEX, name = Название, GGGG, MM, HH, Z, MU, MV, SS, wind)) ## # A tibble: 77,073 x 10 ## INDEX name GGGG MM HH Z MU MV SS wind ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;circular&gt; ## 1 20674 Остров Диксон 2007 1 0 2000 33 31 45 0.8166380 ## 2 20674 Остров Диксон 2007 1 0 1900 32 32 45 0.7853982 ## 3 20674 Остров Диксон 2007 1 0 1800 30 33 46 0.7378151 ## 4 20674 Остров Диксон 2007 1 0 1700 29 35 47 0.6919214 ## 5 20674 Остров Диксон 2007 1 0 1600 28 38 49 0.6350267 ## 6 20674 Остров Диксон 2007 1 0 1500 26 40 50 0.5763752 ## 7 20674 Остров Диксон 2007 1 0 1400 25 41 51 0.5475622 ## 8 20674 Остров Диксон 2007 1 0 1300 25 42 54 0.5369107 ## 9 20674 Остров Диксон 2007 1 0 1200 24 45 56 0.4899573 ## 10 20674 Остров Диксон 2007 1 0 1100 24 49 58 0.4554511 ## # … with 77,063 more rows Выберем данные по высоте 0 метров за 12 часов дня для поселка Тикси, сохранив только составляющие скорости и ее скалярную величину: (tiksi_wind = winds %&gt;% filter(name == &#39;Тикси&#39;, HH == 12, Z == 0)) ## # A tibble: 136 x 10 ## INDEX name GGGG MM HH Z MU MV SS wind ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;circular&gt; ## 1 21824 Тикси 2007 1 12 0 36 35 48 0.7994817 ## 2 21824 Тикси 2007 2 12 0 16 11 27 0.9685090 ## 3 21824 Тикси 2007 3 12 0 23 22 34 0.8076167 ## 4 21824 Тикси 2007 4 12 0 17 9 30 1.0838971 ## 5 21824 Тикси 2007 5 12 0 -17 -3 34 4.5377168 ## 6 21824 Тикси 2007 6 12 0 -23 -25 27 3.8853482 ## 7 21824 Тикси 2007 7 12 0 -5 -11 27 3.5682201 ## 8 21824 Тикси 2007 8 12 0 4 5 25 0.6747409 ## 9 21824 Тикси 2007 9 12 0 24 14 34 1.0427219 ## 10 21824 Тикси 2007 10 12 0 41 47 40 0.7173217 ## # … with 126 more rows Отобразим распределение направлений, розу-диаграмму и плотность распределения. Для построени графиков используем функции plot.circular() и rose.diag из пакета circular. Для аппроксимации плотности распределения направлений воспользуемся функцией kern.den.circ() из пакета NPCirc. Эта функция использует функцию плотности распределения фон Мизеса в качестве ядра и по умолчанию разбивает круг на 250 направлений, по которым производится оценка плотности (при необходимости это значение можно изменить в параметре len): plot.circular(tiksi_wind$wind, cex = 0.5, stack = TRUE, sep = 0.035, axes = FALSE, main = &#39;Среднемноголетняя роза ветров в Тикси&#39;, sub = &#39;Измерения за период с 2007 по 2018 г, высота 0 м&#39;) rose.diag(tiksi_wind$wind, bins = 8, col = &#39;gray70&#39;, border = &#39;gray30&#39;, prop = 1, add = TRUE, tick = FALSE, lwd = 0.5) kden = kern.den.circ(tiksi_wind$wind) lines(kden, shrink = 3, # параметр shrink отвечает за масштаб радиус-вектора join = F, col = &#39;steelblue&#39;) Параметр shrink отвечает за масштаб радиус-вектора на графиках из пакета circular. Чем больше его величина, тем сильнее будет сжат график относительно центра круга. Так же как и в случае с обычными данными, плотность распределения удобно использовать для определения модальных направлений, то есть наиболее часто встречающихся. Для этого воспользуемся функцией findpeaks() из пакета pracma: peak = findpeaks(kden$y, sortstr = T)[1,2] # находим индекс самого высокого пика плотности распределения (modal = kden$x[peak]) # извлекаем сам угол ## Circular Data: ## Type = angles ## Units = radians ## Template = geographics ## Modulo = asis ## Zero = 1.570796 ## Rotation = clock ## [1] 0.813786 # раскладываем на составляющие для отрисовки линии xp = sin(modal) yp = cos(modal) plot.circular(tiksi_wind$wind, cex = 0.5, stack = TRUE, sep = 0.035, axes = FALSE, main = &#39;Среднемноголетняя роза ветров в Тикси&#39;, sub = &#39;Измерения за период с 2007 по 2018 г, высота 0 м&#39;) rose.diag(tiksi_wind$wind, bins = 8, col = &#39;gray70&#39;, border = &#39;gray30&#39;, prop = 1, add = TRUE, tick = FALSE, lwd = 0.5) lines(kden, shrink = 3, join = F, col = &#39;steelblue&#39;) lines(c(0, xp), c(0, yp), lwd = 2, col = &#39;orangered&#39;) text(x = 1.4 * xp, y = 1.4 * yp, col = &#39;orangered&#39;, labels = paste0(round(180 * modal / pi, 0), &#39;°&#39;)) # приводим к целым градусам Проведем анализ направлений для всех станций. Для этого рассчитаем функции плотности распределения и разместим их в новом фрейме данных с лист-колонкой. Лист-колонка (list-column) позволяет хранить в ячейках таблицы данные произвольного типа. В частности, используя лист-колонку, вы можете хранить в каждой ячейке не один объект, а множество объектов, например записать в нее вектор. Лист-колонка имеет тип list, и каждая ячейка в этой колонке так же, соответственно, имеет тип list. Что (и в каком количестве) располагать внутри ячейки — уже ваше дело. Лист-колонки оказываются неожиданно удобны в самых разнообразных сценариях, в том числе для представления статистических моделей (соответствующих каждой строке таблицы) и для хранения пространственных данных (об этом — в следующей лекции). Вместо хранения этих данных в отдельных переменных вы можете записать их в ячейки. В приведенном ниже коде мы группируем все измерения по имени аэрологической обсерватории, вычисляем вектор плотности распределения, записываем его в список, и этот список уже помещается функцией summarise() в единственную ячейку столбца kden, соответствующую данной аэрологической станции. Далее полученная лист-колонка используется для нахождения модальных значений (тут оказывается полезно знание функционалов семейства apply): (dens = winds %&gt;% filter(HH == 12, Z == 0) %&gt;% group_by(name) %&gt;% summarise(kden = list(kern.den.circ(wind))) %&gt;% mutate(peak = sapply(kden, function(X) { peak = findpeaks(X$y, sortstr = T)[1,2] X$x[peak] }) ) ) ## # A tibble: 13 x 3 ## name kden peak ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; ## 1 Верхоянск &lt;dnsty.cr&gt; -2.97 ## 2 Жиганск &lt;dnsty.cr&gt; -3.20 ## 3 Кандалакша &lt;dnsty.cr&gt; -0.347 ## 4 Мирный &lt;dnsty.cr&gt; -0.826 ## 5 Мурманск &lt;dnsty.cr&gt; 0.814 ## 6 Нарьян-Мар &lt;dnsty.cr&gt; 1.04 ## 7 Новолазаревская &lt;dnsty.cr&gt; -0.902 ## 8 Норильск &lt;dnsty.cr&gt; -0.877 ## 9 Оленек &lt;dnsty.cr&gt; -3.48 ## 10 Остров Диксон &lt;dnsty.cr&gt; -0.145 ## 11 Салехард &lt;dnsty.cr&gt; -2.82 ## 12 Тикси &lt;dnsty.cr&gt; 0.814 ## 13 Шойна &lt;dnsty.cr&gt; 0.561 После этого построим розы-диаграммы для всех станций. В данном случае оправдано использование обычного цикла, т.к. итераций немного: # устанавливаем параметры компоновки par(mar = c(1,1,1,1), mfrow = c(1,2)) # строим графики в цикле for (obs_name in dens$name) { wind_df = winds %&gt;% filter(name == obs_name, HH == 12, Z == 0) dens_df = dens %&gt;% filter(name == obs_name) modal = dens_df$peak xp = sin(modal) yp = cos(modal) plot.circular(wind_df$wind, shrink = 1.2, cex = 0.5, stack = TRUE, sep = 0.035, axes = FALSE, main = obs_name) rose.diag(wind_df$wind, bins = 8, col = &#39;gray70&#39;, border = &#39;gray30&#39;, prop = 1, add = TRUE, tick = FALSE, lwd = 0.5) lines(dens_df$kden[[1]], shrink = 3, join=F, col = &#39;steelblue&#39;) lines(c(0, xp), c(0, yp), lwd = 2, col = &#39;orangered&#39;) text(x = 1.4 * xp, y = 1.4 * yp, col = &#39;orangered&#39;, labels = paste0(round(180 * modal / pi, 0), &#39;°&#39;)) # приводим к целым градусам } Таким образом, мы провели графический и статистический анализ среднемноголетних направлений ветра по данным полярных аэрологических станций России. Выявлены модальные направлений, выполнена аппроксимация функции плотности вероятности направлений ветра. 8.2.3 Статистические тесты 8.2.4 Корреляция и регрессия Существуют методы расчета показателей связи между двумя переменными, по крайней мере одна из которых является циркулярной (или сферической, если положение задается двумя углами). Их можно поделить на три большие группы, в зависимости от того, какая из переменных отвечает за направление: линейная—циркулярная; циркулярная—циркулярная; сферическая—сферическая; 8.3 Временные ряды 8.3.1 Создание и преобразование дат и времени Напомним, что текущее время и дату можно получить с помощью системных функций Sys.Date() и Sys.time(): (date = Sys.Date()) ## [1] &quot;2019-10-24&quot; (time = Sys.time()) ## [1] &quot;2019-10-24 21:34:57 MSK&quot; Полученные объекты имеют типы Date и POSIXct: class(date) ## [1] &quot;Date&quot; class(time) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; Несмотря на то, что время и даты печатаются на экран в виде человекочитаемых строк, их внутреннее представление выражается в количестве дней (для дат) и секунд (для времени в форммате POSIXct) начиная с некоторой точки отсчета. Такой точкой отсчета по умолчанию является начало эпохи UNIX, соответстующее \\(1\\) января \\(1970\\) года по гринвичскому (UTC) времени. Чтобы убедиться в этом, воспользуемся функцией difftime, доступной в базовом R: as.integer(date) ## [1] 18193 difftime(date, as.Date(&#39;1970-01-01&#39;)) ## Time difference of 18193 days as.integer(time) ## [1] 1571942097 difftime(time, as.POSIXct(&#39;1970-01-01 00:00:00&#39;, tz = &#39;UTC&#39;), units = &#39;secs&#39;) ## Time difference of 1571942097 secs Работа с датами и временем может быть достаточно утомительной при отсутствии специализированных средств. Пакет lubridate(Spinu, Grolemund, and Wickham 2018) значительно облегчает эту работу. Основные функции lubridate включают синтаксический разбор (“парсинг”) дат в разных форматах, извлечение разных компонент даты и времени (секунд, минут, часов, суток, недель, годов), вычисление разностей и периодов, а также множество вспомогательных функций (хелперов), облегачающих преобразование временных данных. Рассмотрим базовые возможности пакета на нескольких примерах. Создание дат возможно на основе целочисленных и строковых значений: library(lubridate) ymd(20150515) ## [1] &quot;2015-05-15&quot; dmy(&#39;15052015&#39;) ## [1] &quot;2015-05-15&quot; Для создания отметки времени необходимо сформировать строку, которая можети быть интерпретирована должным образом. При необходимости указывается часовой пояс: ymd_hms(&#39;2015-05-15 22:15:34&#39;) # по умолчанию Гринвичское время ## [1] &quot;2015-05-15 22:15:34 UTC&quot; ymd_hms(&#39;2015-05-15 22:15:34&#39;, tz = &quot;Europe/Moscow&quot;) ## [1] &quot;2015-05-15 22:15:34 MSK&quot; Извлечение компоненты даты/времени — одна из самых удобных и востребованных функций lubridate. С помощью этих функций вы можете вытащить из объекта год, месяц, неделю, день, час и секунду: year(time) ## [1] 2019 month(time) ## [1] 10 week(time) ## [1] 43 day(time) ## [1] 24 hour(time) ## [1] 21 second(time) ## [1] 57.01979 Обратите внимание на то, что недели отсчитываются от начала года, а не месяца. Отдельно следует отметить функцию yday(), которая позволяет определить номер дня в году: yday(date) ## [1] 297 Замена компонент даты/времени осуществляется с использованием тех же функций. Например, если мы хотим то же число и время, но за другой (заранее известный) год и месяц, мы можем заменить соответствующие компоненты, используя оператор &lt;-: year(time) &lt;- 2015 month(time) &lt;- 01 time ## [1] &quot;2015-01-24 21:34:57 MSK&quot; Округление дат и времени выполняется с помощью функций round_date(), floor_date() и ceiling_date() соответственно. Например, получить первый день в текущем году можно так: floor_date(Sys.Date(), unit = &#39;year&#39;) ## [1] &quot;2019-01-01&quot; Периоды (periods) — это промежутки дат, выраженные в годах, месяцах или днях. Их удобно использовать для того чтобы сместить текущую дату на заданный интервал. Например, к ранее определенной дате можно прибавить 1 год, 4 месяца, 3 недели и 2 дня: date ## [1] &quot;2019-10-24&quot; date + years(1) + months(4) + weeks(3) + days(2) ## [1] &quot;2021-03-19&quot; Длительности (durations) — это промежутки времени, выраженные в секундах. Работают они в целом аналогично периодам: dweeks(1) ## [1] &quot;604800s (~1 weeks)&quot; time ## [1] &quot;2015-01-24 21:34:57 MSK&quot; time + dweeks(1) ## [1] &quot;2015-01-31 21:34:57 MSK&quot; time + weeks(1) ## [1] &quot;2015-01-31 21:34:57 MSK&quot; Интервалы — это отрезки между двумя датами. Интервал можно преобразовывать в периоды и длительности: (int = interval(Sys.time(), time)) ## [1] 2019-10-24 21:34:57 MSK--2015-01-24 21:34:57 MSK as.period(int, &#39;days&#39;) ## [1] &quot;-1734d 0H 0M -0.101938009262085S&quot; as.duration(int) ## [1] &quot;149817600.101938s (~4.75 years)&quot; 8.3.2 Восстановление пропусков и проверка корректности дат Данные во временных рядах часто соответствуют равноотстоящим по времени срезам (раз в несколько часов, раз в день, раз в три месяца и т.д.), что обусловлено регулярным характером сбора информации (наблюдения, предоставление отчетности и т.д.). Соответствующее предположение лежит и в основе многих функций временного анализа (таких как автокорреляционная функция). Если в данных отсутствуют некоторые временные срезы, это нарушает регулярность временного ряда, что может привести к его некорректной интерпретации. Необходимо восстановить пропущенные сроки, явным образом указав, что данных на эти сроки нет. Помимо этого, дата может быть записана в некорректной форме. Например, оператор ввода данных перепутал месяц и день 18 марта, что привело к созданию несуществующей даты 03.18 в одной из строк. Подобные несовершенства временных рядов важно выявить на самых ранних стадиях анализа данных. Рассмотрим, как эту задачу можно решить средствами R. В качестве источника данных будем использовать данные1 об уровне воды на гидропосте Паялка (р. Умба, Мурманская область) с 1932 по 2014 год. Отсутствие информации в файле данных закодировано числом 9999: (src = read_delim(&#39;data/in_Umba.txt&#39;, delim = &#39; &#39;, col_names = c(&#39;day&#39;, &#39;month&#39;, &#39;year&#39;, &#39;level&#39;), na = &#39;9999&#39;)) ## # A tibble: 30,316 x 4 ## day month year level ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1932 49.4 ## 2 2 1 1932 NA ## 3 3 1 1932 NA ## 4 4 1 1932 NA ## 5 5 1 1932 NA ## 6 6 1 1932 NA ## 7 7 1 1932 NA ## 8 8 1 1932 NA ## 9 9 1 1932 NA ## 10 10 1 1932 47 ## # … with 30,306 more rows Сформируем даты на основе первых трёх столбцов и проверим, все ли из них корректны. Если компоненты даты некорректны, то функция yms() вернет NA: tab = src %&gt;% mutate(Date = ymd(paste(year, month, day))) tab %&gt;% filter(is.na(Date)) ## # A tibble: 1 x 5 ## day month year level Date ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; ## 1 29 2 1941 27.4 NA Проверка показала, что преобразование в дату оказалось невозможно только для одной строки. В этой строке оператором была введена несуществующая дата — 29 февраля 1941 года. Этот год не является високосным, а значит количество дней в феврале должно равняться 28. Единственный выход в данной ситуациии — отбраковать запись: tab = tab %&gt;% filter(!is.na(Date)) После того как мы убедились, что в таблице присутствуют только корректные даты, можно проверять пропуски дат. Для этого упорядочим таблицу по возрастанию дат и используем функцию lag() из пакета dplyr, чтобы вычислить разницу между каждой датой и ее предшественницей. После этого отфильтруем строки, которым предшествует 1 или более пропущенных дней и сформируем отчетную таблицу, содержащую все найденные пропуски: tab %&gt;% arrange(Date) %&gt;% mutate(Gap = Date - lag(Date, 1) - 1) %&gt;% filter(Gap &gt; 0) %&gt;% transmute(start_date = Date - Gap, end_date = Date - 1, duration = Gap) ## # A tibble: 1 x 3 ## start_date end_date duration ## &lt;date&gt; &lt;date&gt; &lt;drtn&gt; ## 1 1941-12-31 1942-01-01 2 days Данная таблица говорит нам о том, что имеется один пропуск из двух дат: 31 декабря 1941 года и 01 января 1942 года. Чтобы восстановить пропущенные сроки, воспользуемся функцией complete() из пакета tidyr, указав ей, что переменная Date должна содержать все даты с шагом в один день, начиная с самой ранней и заканчивая самой поздней: tab_compl = tab %&gt;% complete(Date = seq(min(Date, na.rm = T), max(Date, na.rm = T), by = &#39;day&#39;)) Итак, полная процедура проверки корректности и восстановления пропущенных дат в виде одного конвейера манипуляций будет выглядеть следующим образом: tab = src %&gt;% mutate(Date = ymd(paste(year, month, day))) %&gt;% filter(!is.na(Date)) %&gt;% complete(Date = seq(min(Date, na.rm = T), max(Date, na.rm = T), by = &#39;day&#39;)) 8.3.3 Интерполяция по времени Одна из распространенных задач при работе с временными данными — это интерполяция по времени. Во-первых, интерполяция может использоваться для заполнения пропусков данных. Во-вторых, необходимость в интерполяции возникает когда неравномерно распределенные по времени данные надо перенести на регулярные сроки (скажем, через час), чтобы обеспечить их сравнимость с другими рядами данных. Заметим, что и в том и в другом случае необходимо учитывать автокорреляционные свойства временного ряда и с осторожностью подходить к интерполяции на длительных промежутках времени, поскольку такая интерполяция может не иметь под собой физических оснований. 8.3.3.1 Заполнение пропусков Рассмотрим заполнение пропусков данных на примере загруженных в предыдущем параграфе данных по уровням воды на гидропосте Паялка. На первом этапе анализа пропусков данных целесообразно получить сводную таблицу, которая бы систематизировала все пропуски и непрерывные ряды данных. Для этого сначала выставим маркер data/gap (данные/пропуск) на против каждой строки в новом поле type, а затем пронумеруем все группы последовательно идущих друг за другом меток совпадающего типа. Для реализации последнего шага выполним следующее: сформируем группы непрерывно идущих следом друг за другом меток одного типа, используя функцию rle (run-length encoding); полученный объект содержит вектор lengths, количество элементов которого равняется количеству групп, а значение каждого элемента равно количеству объектов соответствующей по порядку группы; номер каждой группы (от 1 до количества групп) продублируем столько раз, сколько элементов содержится в каждой группе После этого сгруппируем данные по номеру группы и вычислим дату начала, дату конца, продолжительность и тип каждого периода. Полученная таблица наглядно демонстрирует разбиение временного ряда на периоды наличия и отсутствия данных: timerep = tab %&gt;% mutate(type = if_else(is.na(level), &#39;gap&#39;, &#39;data&#39;), num = with(rle(type), rep(seq_along(lengths), lengths))) %&gt;% group_by(num) %&gt;% summarise(start_date = min(Date), end_date = max(Date), duration = end_date - start_date + 1, type = first(type)) Путём интерполяции можно заполнить все пропуски в данном ряду, однако достоверность (правдоподобие) интерполяции будет снижаться при увеличении длины пропуска. Критическую длину пропуска целесообразно связать с пороговым значением автокорреляции — коэффициента корреляции исходного ряда данных и его копии, полученной со сдвигом \\(\\tau\\). Автокорреляцию как правило рассчитываеют не при фиксированном сдвиге, а для серии сдвигов. Полученная функция показывает зависимость автокорреляции от величины сдвига и носит название автокорреляционной функции (АКФ): \\[\\Psi(\\tau) = \\int_{-\\infty}^{+\\infty} f(t) f^* (t - \\tau) dt,\\] где \\(^*\\) означает комплексное сопряжение (для вещественнозначных функций эту звездочку можно игнорировать, она нужна в целях обобщения понятия автокорреляции для случайных процессов, сечения которых являются комплексными случайными переменными). Автокорреляционная функция случайного процесса \\(X(t)\\) будет иметь вид: \\[K(\\tau) = \\mathbb E \\big[X(t) X^* (t - \\tau) \\big],\\] где \\(\\mathbb E \\big[~ \\big]\\) — математическое ожидание. Для нецикличных процессов, плавно изменяющихся во времени, при увеличении \\(\\tau\\) значение АКФ падает, а это означает, что установив минимально допустимое значение автокорреляции, можно выяснить соответствующий ему сдвиг по времени. Найденная величина и будет максимально допустимой при интерполяции длиной пропуска. Для вычисления АКФ найдем сначала максимальный период непрерывных наблюдений: (max_period = filter(timerep, type == &#39;data&#39;, duration == max(duration))) ## # A tibble: 1 x 5 ## num start_date end_date duration type ## &lt;int&gt; &lt;date&gt; &lt;date&gt; &lt;drtn&gt; &lt;chr&gt; ## 1 97 1946-01-01 1980-12-31 12784 days data После этого отфильтруем данные на найденный период и вычислим АКФ, используя встроенную в базовый R функцию acf(): par(mar = c(6,5,4,2)) autocorr = tab %&gt;% filter(between(Date, max_period$start_date, max_period$end_date)) %&gt;% pull(level) %&gt;% acf() Результат соответствует нашим ожиданиям: автокорреляционная функция монотонно убывает при увеличении сдвига по времени. Осталось устновить пороговое значение автокорреляции и найти соответствующий ему сдвиг. В гидрологии за допустимую величину автокорреляции при восстановлении рядов данных принято брать значение, равное \\(0.7\\). Найдем индекс первого элемента менее данной величины, используя функцию detect_index() из пакета purrr: (max_dur = purrr::detect_index(autocorr$acf, ~ .x &lt; 0.7)) ## [1] 15 Полученное значение говорит нам о том, что при заданном допуске допустимо интерполировать значения в пропусках данных короче, чем 15 дней. Для выполнения интерполяции воспользуемся функцией na.approx() из пакета zoo и округлим полученные значения до одного знака после запятой (что соответствует точности исходных данных): (tab_interp = tab %&gt;% mutate(level_interp = zoo::na.approx(level, maxgap = max_dur) %&gt;% round(1))) ## # A tibble: 30,317 x 6 ## Date day month year level level_interp ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1932-01-01 1 1 1932 49.4 49.4 ## 2 1932-01-02 2 1 1932 NA 49.1 ## 3 1932-01-03 3 1 1932 NA 48.9 ## 4 1932-01-04 4 1 1932 NA 48.6 ## 5 1932-01-05 5 1 1932 NA 48.3 ## 6 1932-01-06 6 1 1932 NA 48.1 ## 7 1932-01-07 7 1 1932 NA 47.8 ## 8 1932-01-08 8 1 1932 NA 47.5 ## 9 1932-01-09 9 1 1932 NA 47.3 ## 10 1932-01-10 10 1 1932 47 47 ## # … with 30,307 more rows В заключение проведем заново оценку ситуации с пропусками в данных timerep_interp = tab_interp %&gt;% mutate(type = if_else(is.na(level_interp), &#39;gap&#39;, &#39;data&#39;), num = with(rle(type), rep(seq_along(lengths), lengths))) %&gt;% group_by(num) %&gt;% summarise(start_date = min(Date), end_date = max(Date), duration = end_date - start_date + 1, type = first(type)) По результатам автокореляционного анализа и интерполяции удалось заполнить значительное число пропусков в данных. Однако по прежнему остаются значительные по длине пропуски, которые уже требуют привлечения дополнительных источников информации для их заполнения. 8.3.3.2 Пересчет на другую временную сетку Когда данные, поступающие из различных источников, привязаны к несовпадающим временным срезам, возникает задача приведения их к единой временной сетке. Как правило, эта сетка имеет регулярный шаг (каждый час, каждый месяц и т.д.), поскольку это упрощает выполнение статистического анализа. Одним из источников данных, не привязанных к жесткой временной сетке, является геосенсорная сеть домашних метеостанций NETATMO, которая собирает информацию с пользовательских устройств примерно каждый полчаса. Сроки, однако, четко не соблюдаются. Помимо этого, система, в силу её добровольно-волонтёрского характера, не предусматривает бесперебойное функционирование всех метеостанций: пользователь может отключить свой прибор на несколько часов или дней, в результате чего в данных могут образоваться дополнительные пропуски. Вследствие этого данные NETATMO характеризуются высокой степенью иррегулярности во времени. Загрузим в качестве примера данные по метеостанции с идентификатором 70_ee_50_00_8e_1a, расположенной в пределах Московского мегаполиса (данные выгружены посредством NETATMO Weather API): (tab = read_csv(&#39;data/70_ee_50_00_8e_1a.csv&#39;)) ## # A tibble: 1,405 x 11 ## altitude humidity id latitude longitude pressure temperature ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 189 55 70:e… 55.7 37.6 1014. 23.1 ## 2 189 56 70:e… 55.7 37.6 1013. 22.5 ## 3 189 57 70:e… 55.7 37.6 1014. 21.9 ## 4 189 59 70:e… 55.7 37.6 1014. 21.1 ## 5 189 62 70:e… 55.7 37.6 1014 20 ## 6 189 65 70:e… 55.7 37.6 1014. 19.7 ## 7 189 66 70:e… 55.7 37.6 1014. 19.6 ## 8 189 67 70:e… 55.7 37.6 1014. 19.5 ## 9 189 68 70:e… 55.7 37.6 1014. 19.2 ## 10 189 68 70:e… 55.7 37.6 1015. 19.2 ## # … with 1,395 more rows, and 4 more variables: time_humidity &lt;dttm&gt;, ## # time_pressure &lt;dttm&gt;, time_temperature &lt;dttm&gt;, timezone &lt;chr&gt; Визуальная инспекция данных подсказывает нам, что данные по температуре, влажности и давлению получены на произвольные сроки с дискретностью порядка \\(30\\) или \\(60\\) минут, при этом для всех трех метеопараметров эти сроки не совпадают: Чтобы осуществлять совместный анализ этих данных, необходимо привести их к единой временной сетке. Временная плотность данных NETATMO позволяет сделать такую сетку через каждые \\(30\\) минут. Алгоритм интерполяции данных для каждой из характеристик будет следующий: Определить минимальное и максимальное время измерений и округлить их до ближайшего времени, кратного \\(30\\) минутам в большую (для минимального времени) и меньшую (для максимального времени) сторону. Сформировать последовательность временных срезов между полученными границами \\(30\\)-минутной серии. Интерполировать величину показателя на новую сетку. Определим расчетные интервалы времени для каждой из характеристик: (time_bounds = tibble( type = c(&#39;temperature&#39;, &#39;humidity&#39;, &#39;pressure&#39;), tmin = ceiling_date(c(min(tab$time_temperature), min(tab$time_humidity), min(tab$time_pressure)), unit = &#39;30 minutes&#39;), tmax = floor_date(c(max(tab$time_temperature), max(tab$time_humidity), max(tab$time_pressure)), unit = &#39;30 minutes&#39;) )) ## # A tibble: 3 x 3 ## type tmin tmax ## &lt;chr&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 temperature 2019-09-04 14:30:00 2019-10-17 06:30:00 ## 2 humidity 2019-09-04 14:30:00 2019-10-17 06:30:00 ## 3 pressure 2019-09-04 14:30:00 2019-10-17 06:30:00 В данном случае видно, что возможные границы сроков интерполяции для всех трех переменных совпадают, что несколько облегачает задачу. Проинтерполуруем данные на единую регулярную временную сетку через \\(30\\) минут, используя функцию approx() из базового R. По умолчанию данная функция использует линейную интерполяцию по ближайшим значениям до и после интерполируемого: (time_interp = tibble( datetime = seq(min(time_bounds$tmin), max(time_bounds$tmax), by = &#39;30 min&#39;), temp = round(approx(tab$time_temperature, tab$temperature, xout = datetime)$y, 1), humd = round(approx(tab$time_humidity, tab$humidity, xout = datetime)$y, 1), pres = round(approx(tab$time_pressure, tab$pressure, xout = datetime)$y, 1) )) ## # A tibble: 2,049 x 4 ## datetime temp humd pres ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2019-09-04 14:30:00 22.8 55.5 1014. ## 2 2019-09-04 15:00:00 22.4 56.1 1013. ## 3 2019-09-04 15:30:00 22.1 56.6 1014. ## 4 2019-09-04 16:00:00 21.7 57.5 1014. ## 5 2019-09-04 16:30:00 20.8 59.8 1014. ## 6 2019-09-04 17:00:00 20 62.4 1014 ## 7 2019-09-04 17:30:00 19.8 63.9 1014. ## 8 2019-09-04 18:00:00 19.7 65.2 1014. ## 9 2019-09-04 18:30:00 19.6 66.2 1014. ## 10 2019-09-04 19:00:00 19.5 67.1 1014. ## # … with 2,039 more rows Надежность каждого интерполированного значения можно оценить, задав максимально допустимое расстояние по времени до ближайшего элемента исходных данных до и после интерполируемого времени. Предположим, валидными будут считаться значения, расположенные не далее чем \\(30\\) минут до ближайшего временного среза (до или после), при этом длина интервала, в который попадает интерполируемый срез, не должна превышать \\(90\\) минут. Задачу поиска временного интервала исходных данных, в который попадает каждый интерполируемый временной срез, удобно решить с помощью кусочно-линейной функции stepfun(), передав ей в качестве аргумента \\(X\\) упорядоченные по времени сроки исходных данных, а в качестве значений \\(Y\\) — порядковые номера исходных сроков: src_time = sort(tab$time_temperature) (tempfun = stepfun(src_time, 0:nrow(tab))) ## Step function ## Call: stepfun(src_time, 0:nrow(tab)) ## x[1:1405] = 1.5676e+09, 1.5676e+09, 1.5676e+09, ..., 1.5713e+09, 1.5713e+09 ## 1406 plateau levels = 0, 1, 2, ..., 1404, 1405 Созданная таким образом кусочно-линейная функция для переданного ей значения времени будет возвращать порядковый номер временного интервала исходных данных. Оценим с помощью нее временные расстояния до ближайших наблюдений и их соответствие выдвинутым условиям: (timediff = lapply(time_interp$datetime, function(time) { idx = tempfun(time) tibble(datetime = time, temp_before = time - src_time[idx], temp_after = src_time[idx + 1] - time, temp_valid = min(temp_before, temp_after) &lt;= minutes(30) &amp;&amp; temp_before + temp_after &lt;= minutes(90)) }) %&gt;% bind_rows()) ## # A tibble: 2,049 x 4 ## datetime temp_before temp_after temp_valid ## &lt;dttm&gt; &lt;drtn&gt; &lt;drtn&gt; &lt;lgl&gt; ## 1 2019-09-04 14:30:00 1113 secs 1296 secs TRUE ## 2 2019-09-04 15:00:00 504 secs 3135 secs TRUE ## 3 2019-09-04 15:30:00 2304 secs 1335 secs TRUE ## 4 2019-09-04 16:00:00 465 secs 1330 secs TRUE ## 5 2019-09-04 16:30:00 470 secs 1324 secs TRUE ## 6 2019-09-04 17:00:00 476 secs 3163 secs TRUE ## 7 2019-09-04 17:30:00 2276 secs 1363 secs TRUE ## 8 2019-09-04 18:00:00 437 secs 1358 secs TRUE ## 9 2019-09-04 18:30:00 442 secs 1403 secs TRUE ## 10 2019-09-04 19:00:00 397 secs 3499 secs TRUE ## # … with 2,039 more rows summary(timediff) ## datetime temp_before temp_after ## Min. :2019-09-04 14:30:00 Length:2049 Length:2049 ## 1st Qu.:2019-09-15 06:30:00 Class :difftime Class :difftime ## Median :2019-09-25 22:30:00 Mode :numeric Mode :numeric ## Mean :2019-09-25 22:30:00 ## 3rd Qu.:2019-10-06 14:30:00 ## Max. :2019-10-17 06:30:00 ## temp_valid ## Mode :logical ## FALSE:594 ## TRUE :1455 ## ## ## Полученные результаты говорят нам о том, что критериям соответствуют \\(1455\\) значений температур из \\(2049\\), остальные \\(594\\) значения либо попадают в слишком длинный интервал между исходными данными (более \\(90\\) минут), либо расположены слишком далеко от ближайшего наблюдения (более \\(30\\) минут). В заключение присоединим полученную информацию к таблице интерполированных значений: time_interp_checked = time_interp %&gt;% left_join(timediff, by = c(&#39;datetime&#39; = &#39;datetime&#39;)) Аналогичную проверку валидности следует выполнить для оставшихся переменных — влажности и давления. 8.3.4 Статистики Существует ряд статистик и статистических тестов, которые часто используются для временных данных. Среди них мы рассмотрим следующие: Тест Манна-Кендалла на значимость линейного тренда Тест Петтитт на точку перелома — Оценка тренда по методу Тейла-Сена В качестве примера возьмем данные межгодичных изменений характеристик стока реки Мезень на посту Малонисогорская2: library(readr) (tab = read_csv(&#39;data/Mezen.csv&#39;)) ## # A tibble: 75 x 57 ## year_number Year1 Year2 datestart datepolend Qy Qmax datemax ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; ## 1 1 1938 1939 2000-03-23 2000-06-08 98.1 747 2000-04-03 ## 2 2 1939 1940 2000-03-16 2000-07-11 66.9 487 2000-04-28 ## 3 3 1940 1941 2000-03-09 2000-07-12 97.4 995 2000-04-28 ## 4 4 1941 1942 2000-03-24 2000-07-21 214. 2030 2000-05-01 ## 5 5 1942 1943 2000-04-02 2000-07-31 242. 2790 2000-05-08 ## 6 6 1943 1944 2000-03-24 2000-06-15 71.3 451 2000-05-02 ## 7 7 1944 1945 2000-02-26 2000-06-30 89.0 530 2000-04-28 ## 8 8 1945 1946 2000-03-25 2000-07-13 128. 1220 2000-04-27 ## 9 9 1946 1947 2000-03-15 2000-06-30 179. 2150 2000-04-28 ## 10 10 1947 1948 2000-03-06 2000-07-15 153. 1720 2000-04-16 ## # … with 65 more rows, and 49 more variables: Qygr &lt;dbl&gt;, Qmmsummer &lt;dbl&gt;, ## # monmmsummer &lt;date&gt;, Qmmwin &lt;dbl&gt;, nommwin &lt;date&gt;, Q30s &lt;dbl&gt;, ## # date30s1 &lt;date&gt;, date30s2 &lt;date&gt;, Q30w &lt;dbl&gt;, date30w1 &lt;date&gt;, ## # date30w2 &lt;date&gt;, Q10s &lt;dbl&gt;, date10s1 &lt;date&gt;, date10s2 &lt;date&gt;, ## # Q10w &lt;dbl&gt;, date10w1 &lt;date&gt;, date10w2 &lt;date&gt;, Q5s &lt;dbl&gt;, ## # date5s1 &lt;date&gt;, date5s2 &lt;date&gt;, Q5w &lt;dbl&gt;, date5w1 &lt;date&gt;, ## # date5w2 &lt;date&gt;, Wy &lt;dbl&gt;, Wgr &lt;dbl&gt;, Wpol1 &lt;dbl&gt;, Wpol2 &lt;dbl&gt;, ## # Wpol3 &lt;dbl&gt;, Wpavs1 &lt;dbl&gt;, Wpavs2 &lt;dbl&gt;, Wpavthaw1 &lt;dbl&gt;, ## # Wpavthaw2 &lt;dbl&gt;, WgrS &lt;dbl&gt;, WS &lt;dbl&gt;, WgrW &lt;dbl&gt;, WW &lt;dbl&gt;, ## # Qmaxpavs &lt;dbl&gt;, datemaxpavs &lt;date&gt;, Qmaxpavthaw &lt;dbl&gt;, ## # datemaxpavthaw &lt;date&gt;, SumProd &lt;dbl&gt;, DaysPavsSum &lt;dbl&gt;, ## # WinProd &lt;dbl&gt;, DaysThawWin &lt;dbl&gt;, CvWin &lt;dbl&gt;, CvSum &lt;dbl&gt;, ## # CountPavs &lt;dbl&gt;, CountThaws &lt;dbl&gt;, PolProd &lt;dbl&gt; Построим график межгодичной изменчивости объема грунтового стока (переменная Wgr в \\(км^3\\)): ggplot(tab, mapping = aes(Year1, Wgr)) + geom_line() + geom_area(alpha = 0.5) + geom_smooth() + labs(title = &#39;Объем грунтового стока на р. Мезень в д. Малонисогорская&#39;, x = &#39;Год&#39;, y = &#39;куб. км&#39;) Для выполнения тестов Манна-Кендалла, Петтитт и оценки тренда по методу Тейла-Сена подключим пакет trend: library(trend) (mk = mk.test(tab$Wgr)) ## ## Mann-Kendall trend test ## ## data: tab$Wgr ## z = 7.8129, n = 75, p-value = 5.589e-15 ## alternative hypothesis: true S is not equal to 0 ## sample estimates: ## S varS tau ## 1.709000e+03 4.779167e+04 6.158559e-01 (pt = pettitt.test(tab$Wgr)) ## ## Pettitt&#39;s test for single change-point detection ## ## data: tab$Wgr ## U* = 1324, p-value = 4.131e-11 ## alternative hypothesis: two.sided ## sample estimates: ## probable change point at time K ## 38 (ts = sens.slope(tab$Wgr)) ## ## Sen&#39;s slope ## ## data: tab$Wgr ## z = 7.8129, n = 75, p-value = 5.589e-15 ## alternative hypothesis: true z is not equal to 0 ## 95 percent confidence interval: ## 0.01378903 0.02202750 ## sample estimates: ## Sen&#39;s slope ## 0.01810404 Все три теста в данном случе дают высокую статистическую значимость временных изменений (p-значения), при этом тест Петтитт говорит, что точка перелома находится в 38-й позиции ряда. Если разделить исследуемый временной ряд на две выборки этой точкой, то они будут иметь статистически значимое отличие в характеристиках среднего значения показателя. Метод Тейла-Сена также говорит нам, что грунтовый сток увеличивается ежегодно примерно на \\(1.8\\%\\) (величина тренда равна \\(0.0181\\)), что за период 70 лет даёт абсолютный прирост грунтового стока более чем в 2 раза. Для наглядности нанесем линию тренда и точку перелома на график: ggplot(tab, mapping = aes(Year1, Wgr)) + geom_line() + geom_area(alpha = 0.5) + geom_smooth(method = &#39;lm&#39;, color = &#39;red&#39;) + geom_vline(xintercept = tab$Year1[pt$estimate], color = &quot;red&quot;, size = 0.5) + annotate(&quot;text&quot;, label = tab$Year1[pt$estimate], x = tab$Year1[pt$estimate] + 4, y = max(tab$Wgr), size = 4, colour = &quot;red&quot;) + labs(title = &#39;Объем грунтового стока на р. Мезень в д. Малонисогорская&#39;, x = &#39;Год&#39;, y = &#39;куб. км&#39;) 8.3.5 Анимация Анимационная графика возволяет наглядно визуализировать изменения. Наиболее часто речь идет об изменениях по времени. В этом случае время работает в роли невидимой переменной, которая влияет на положение графических примитивов на изображении. Данный подход органично вписывается в концепцию грамматики графики, на основе которой построен пакет ggplot2 (см. Главу 6). Соответствующую реализацию грамматики анимаций предоставляет пакет gganimate(Pedersen and Robinson 2019). Возможности анимаций в gganimate реализуются посредством добавления новых грамматик к построенному графику ggplot2. К числу этих грамматик относятся: transition_*() — распределение данных по времени; view_*() — поведение осей координат во времени; shadow_*() — отображение данных, не относящихся к текущему временному срезу; enter_*()/exit_*() — характер появления/исчезновения данных в процессе анимации; ease_aes() — порядок смягчения (интерполяции) графических переменных в моменты перехода. В качестве первого примера используем уже знакомые нам данные реанализа NASA POWER суточного осреднения, выгрузив информацию по точкам в трех городах (Мурманск, Москва, Краснодар) за 2018 год: library(nasapower) library(ggplot2) cities = list( Мурманск = c(33, 69), Москва = c(38, 56), Краснодар = c(39, 45) ) tab = purrr::imap(cities, function(coords, city){ get_power( community = &quot;AG&quot;, lonlat = coords, pars = c(&quot;RH2M&quot;, &quot;T2M&quot;, &quot;PRECTOT&quot;), dates = c(&quot;2018-01-01&quot;, &quot;2018-12-31&quot;), temporal_average = &quot;DAILY&quot; ) %&gt;% mutate(CITY = city, MONTH = month(YYYYMMDD)) }) %&gt;% bind_rows() 8.3.5.1 Переход по времени Рассмотрим колебания температуры по 12 месяцам посредством диаграммы размаха, реализовав анимационный переход по времени посредством функции transition_time(). Текущий временной срез передается в переменную окружения frame_time, которая подается в подзаголовок графика (см параметр subtitle функции labs()): ggplot(tab, aes(CITY, T2M)) + geom_boxplot() + labs(title = &quot;Температура воздуха в 2018 году по данным NASA POWER&quot;, subtitle = &#39;Месяц: {round(frame_time)}&#39;) + xlab(&#39;Город&#39;) + ylab(&#39;Т, °С&#39;) + transition_time(MONTH) Текущий срез при выполнении анимации по времени доступен в переменной окружения frame_time. Аналогичную анимацию можно провести и на примере функции плотности распределения: ggplot(tab, aes(T2M, fill = CITY)) + geom_density(alpha = 0.5) + labs(title = &quot;Температура воздуха в 2018 году по данным NASA POWER&quot;, subtitle = &#39;Месяц: {round(frame_time)}&#39;, fill = &#39;Город&#39;) + xlab(&#39;Т, °С&#39;) + ylab(&#39;Плотность распределения&#39;) + transition_time(MONTH) Загрузим ранее использованные в Главе @ref(stat_analysis) данные Gapminder по соотношению продолжительности жизни и ВВП на душу населения, но на этот раз не будем фильтровать их по времени: library(readxl) library(googledrive) library(googlesheets4) countries = read_excel(&#39;data/gapminder.xlsx&#39;, 2) %&gt;% select(Country = name, Region = eight_regions) %&gt;% mutate(Country = factor(Country, levels = Country[order(.$Region)])) gdpdf_tidy = &#39;1cxtzRRN6ldjSGoDzFHkB8vqPavq1iOTMElGewQnmHgg&#39; %&gt;% ### ВВП на душу населения as_id() %&gt;% # преобразуем идентификатор в класс drive_id чтобы отличать его от пути drive_get() %&gt;% read_sheet() %&gt;% pivot_longer(cols = `1764`:`2018`, names_to = &#39;year&#39;, values_to = &#39;gdp&#39;) %&gt;% rename(Country = 1) popdf_tidy = &#39;1IbDM8z5XicMIXgr93FPwjgwoTTKMuyLfzU6cQrGZzH8&#39; %&gt;% # численность населения as_id() %&gt;% # преобразуем идентификатор в класс drive_id чтобы отличать его от пути drive_get() %&gt;% read_sheet() %&gt;% # первый лист pivot_longer(cols = `1800`:`2015`, names_to = &#39;year&#39;, values_to = &#39;pop&#39;) %&gt;% rename(Country = 1) lifedf_tidy = &#39;1H3nzTwbn8z4lJ5gJ_WfDgCeGEXK3PVGcNjQ_U5og8eo&#39; %&gt;% # продолжительность жизни as_id() %&gt;% # преобразуем идентификатор в класс drive_id чтобы отличать его от пути drive_get() %&gt;% read_sheet() %&gt;% pivot_longer(cols = `1800`:`2016`, names_to = &#39;year&#39;, values_to = &#39;lifexp&#39;) %&gt;% rename(Country = 1) tab = gdpdf_tidy %&gt;% inner_join(lifedf_tidy) %&gt;% inner_join(popdf_tidy) %&gt;% inner_join(countries) %&gt;% mutate(year = as.integer(year)) %&gt;% drop_na() Теперь чтобы отобразить это соотношение в виде анимации, достаточно добавить новый переход по времени: options(scipen = 999) # убираем экспоненциальную форму записи числа ggplot(tab, aes(gdp, lifexp, size = pop, color = Region)) + geom_point(alpha = 0.5) + scale_x_log10() + labs(title = &#39;Year: {round(frame_time)}&#39;) + theme_bw() + transition_time(year) 8.3.5.2 Переход по состояниям В ряде случаев вместо перехода по времени целесообразно использовать переход по состояниям. В частности, такой подход оказывается удобен, когда сопоставляются данные за аналогичные временные срезы разных периодов. Например, 12 часов каждого дня недели с анимацией по неделям. Либо каждый день года с анимацией по годам. В этом случае координаты X будут фиксированы, а значение Y будет зависить от текущего состояния. Подобную стратегию можно использовать для визуализации изменений внутригодичного распределения величины между годами. Примером таких изменений являются данные о расходах воды на гидропосте Паялка, рассмотренные нами ранее в настоящей главе. В качестве результата мы хотим видеть анимацию гидрографа реки, в которой каждый кадр соответствует календарному году. Для составления такой анимации необходимо сначала убедиться, что в данных все года заполнены корректно (не пусты), и что нет годов, за которые вообще нет данных (такие года придется из анимации исключить, так как для них гидрограф построить невозможно). Помимо этого, чтобы обеспечить сопоставимость аналогичных дат за разные года, необходимо сохранить у них только месяц и день. Поскольку такого формата даты не существует, в качестве “трюка” можно просто заменить года всех дат на \\(2000\\) и записать результат в новое поле. Необходимые преобразования реализуются следующим образом: flt_tab = tab_interp %&gt;% filter(!is.na(year)) %&gt;% group_by(year) %&gt;% filter(!all(is.na(level_interp))) %&gt;% ungroup() %&gt;% mutate(yDate = Date) year(flt_tab$yDate) &lt;- 2000 # фиктивное поле, в котором аналогичные даты за разные года совпадают После выполнения необходимой подготовки таблица данных готова для анимации. Переход по состояниям реализуется посредством вызова функции transition_states(), при этом параметр state_length = 0 обеспечивает плавность анимации за счет нулевой задержки в каждом состоянии. Полученный график предварительно сохраняется в промежуточную переменную anim, чтобы в дальнейшем можно было управлять качеством анимации путем вызова функции animate(). В частности, мы устанавливаем общее число кадров в \\(10\\) раз больше количества состояний (годов), чтобы обеспечить плавный переход между ними посредством интерполяции (tweening), автоматически выполняемой функцией animate() между кадрами: anim = ggplot(flt_tab, mapping = aes(x = yDate, y = level_interp)) + geom_ribbon(aes(ymin = 0, ymax = level_interp), alpha = 0.5) + geom_line() + scale_x_date(date_breaks = &quot;1 month&quot;, date_labels = &quot;%b&quot;) + labs(title = &quot;Расход воды на гидропосту Паялка (р. Умба, Мурманская обл.)&quot;, subtitle = &#39;Год: {closest_state}&#39;) + xlab(&#39;Дата&#39;) + ylab(&#39;куб.м/с&#39;) + theme(text = element_text(size = 18, family = &#39;Open Sans&#39;)) + transition_states(year, state_length = 0) + view_follow(fixed_y = TRUE) animate(anim, fps = 20, # число кадров в секунду nframes = 10 * length(unique(flt_tab$year)), # общее число кадров width = 800, height = 600) Текущий срез при выполнении анимации по состояниям доступен в переменной окружения closest_state. В настоящей главе мы рассмотрели лишь базовые возможности создания анимаций средствами пакета gganimate. Более подробно с другими возможностями пакета можно ознакомиться в справочнике по его функциям. 8.4 Контрольные вопросы и упражнения 8.4.1 Вопросы В каком виде направления рассматриваются в круговой статистике? Какое распределение является аналогом нормального распределения для круговых данных? Что означают параметры \\(\\kappa\\) и \\(\\mu\\) в функции этого распределения? Как вычисляется равнодействующий вектор первого порядка и выборочная средняя равнодействующая длина этого вектора для направлений? Чем аксиальные данные отличаются от круговых данных в общем случае? Какие преобразования осуществляются над такими данными для того чтобы применять к ним стандартные методы циркулярной статистики? Какой класс данных (и пакет) можно использовать в R для представления направлений? Как указать, что направления отсчитываются географическим методом, то есть, по часовой стрелки от направления на север? Какую функцию можно использовать для для оценки плотности распределения круговых данных? В каком пакете она находится? Какую функцию можно использовать для выявления модальных направлений по данным функции плотности вероятности? Какие функции позволяют строить диаграммы и розы-диаграммы по круговым данным в среде R? Какой параметр управляет масштабом радиус-вектора на круговых графиках? Что такое лист-колонка в фрейме данных, и какого типа данные можно в ней хранить? 8.4.2 Упражнения Загрузите файл с данными по межгодичным изменениям стока на реке Мезень. Проанализируйте величину и значимость тренда, а также наличие переломного года для характеристик Wpol2 (объем половодья без грунтовой составляющей), а также Wy (суммарный сток). Что можно сказать об их соотношении с грунтовым стоком? Повторите эксперимент с построением анимационных графиков функции плотности распределения температуры и диаграмм размаха по данным NASA POWER (параграф @ref(circular_time_animation)), но выбрав координаты трех других географических локаций. Самсонов Т.Е. Визуализация и анализ географических данных на языке R. М.: Географический факультет МГУ, 2019. DOI: 10.5281/zenodo.901911 References "]
]
